#########################################
# Logstash CSV Config File				#
# To Read EVTX converted to CSV			#
# 20161102								#
# Windows Event Logs [Security Logs]	#
# JERIEL JUARBE							#
#########################################

#########################
#						#	
#	SecurityLogs INPUT	#
#						#
#########################
input {
  file {
    type => "SecurityLogs"
    start_position => "beginning"
    ignore_older => "432000"
    sincedb_path => "C:\Users\grassmarlin1\Desktop\Juarbe-Milbank\since_db\.securitylogs"

    #Edit the following path to reflect the location of your log files. You can also change the extension if you use something else
    path => "C:\Users\grassmarlin1\Desktop\CPT_23\RADF_DC1\2016-05\*.csv"
    tags => [ "DC1" ]
    tags => [ "May_2016" ]
  }
}

#=-=--=-=-=-=-=-=-=-==--=-==-=-=-=-=#
#									#
#	Windows SecurityLogs FILTER		#
#									#
#-=---=-=-=-=-=-=-=-=-=-=-=-=--=-=--#
filter {
	#Now, using the csv filter, we can define the Bro log fields
	if [type] == "SecurityLogs" {
	############PREPROCESSING
		#Let's get rid of the top header lines and all the garbage; they Contains EventLog
		if [message] =~ "^Event" {
			drop { }
		}
		#This looks for errors in each newline; if the C from C:\directory, then process as an error
		if [message] !~ "^[A-Z]:" {
			mutate {
				add_tag => ["error"]
				gsub => [ "message", "[\"\r\n\t]", "." ]
				add_field => {"variable" => "%{message}"}
			 }
		}
	###############CSV COLUMUNIZING	
		######PIPELINE FOR CORRECTLY FORMATED LINES
		if "error" not in [tags] {
			#this is to get rid of any " quotes because it breaks the CSV plugin and converts them to a period .
			mutate {
				gsub => [ "message", "[\"\r\n]", "." ]
			}
			csv {
				columns => ["EventLog","RecordNumber","TimeGenerated","TimeWritten","EventID","EventType","EventTypeName","EventCategory","EventCategoryName","SourceName","Strings","ComputerName","SID","Message1","Data"]
			}
			##Let's convert our timestamp into the 'ts' field, so we can use Kibana features natively
			date {
				match => [ "TimeGenerated", "yyyy-MM-dd HH:mm:ss" ]
		
			}
			##another timestamp exists in the CSV file
			date {
				match => [ "TimeWritten", "yyyy-MM-dd HH:mm:ss" ]
				target => "TimeWritten" 
			}
			## This Mutate is to splite the Strings Field that is PIPE delimeted to use the fields for Analysis in Kibana
			mutate {
				split => {"Strings" => "|"}
				add_field => {"strings0" => "%{[Strings][0]}"}
				add_field => {"strings1" => "%{[Strings][1]}"}
				add_field => {"strings2" => "%{[Strings][2]}"}
				add_field => {"strings3" => "%{[Strings][3]}"}
				add_field => {"strings4" => "%{[Strings][4]}"}
				add_field => {"strings5" => "%{[Strings][5]}"}
				add_field => {"strings6" => "%{[Strings][6]}"}
				add_field => {"strings7" => "%{[Strings][7]}"}
				add_field => {"strings8" => "%{[Strings][8]}"}
				add_field => {"strings9" => "%{[Strings][9]}"}
				add_field => {"strings010" => "%{[Strings][10]}"}
				add_field => {"strings011" => "%{[Strings][11]}"}
				add_field => {"strings012" => "%{[Strings][12]}"}
				add_field => {"strings013" => "%{[Strings][13]}"}
				add_field => {"strings014" => "%{[Strings][14]}"}
				add_field => {"strings015" => "%{[Strings][15]}"}
				add_field => {"strings016" => "%{[Strings][16]}"}
				add_field => {"strings017" => "%{[Strings][17]}"}
				add_field => {"strings018" => "%{[Strings][18]}"}
				add_field => {"strings019" => "%{[Strings][19]}"}
				add_field => {"strings020" => "%{[Strings][20]}"}
				##Removing the SID Column because it's empty for all events
				remove_field => ["SID"]
				}

				## Remove unwanted Fields (retarded long but necessary IF statements
				if "%{[Strings][20]}" in [strings020] {
					mutate {
						remove_field => ["strings020"]
					}
				}
				if "%{[Strings][19]}" in [strings019] {
					mutate {
						remove_field => ["strings019"]
					}
				}
				if "%{[Strings][18]}" in [strings018] {
					mutate {
						remove_field => ["strings018"]
					}
				}
				if "%{[Strings][17]}" in [strings017] {
					mutate {
						remove_field => ["strings017"]
					}
				}
				if "%{[Strings][16]}" in [strings016] {
					mutate {
						remove_field => ["strings016"]
					}
				}
				if "%{[Strings][15]}" in [strings015] {
					mutate {
						remove_field => ["strings015"]
					}
				}
				if "%{[Strings][14]}" in [strings014] {
					mutate {
						remove_field => ["strings014"]
					}
				}
				if "%{[Strings][13]}" in [strings013] {
					mutate {
						remove_field => ["strings013"]
					}
				}
				if "%{[Strings][12]}" in [strings012] {
					mutate {
						remove_field => ["strings012"]
					}
				}
				if "%{[Strings][11]}" in [strings011] {
					mutate {
						remove_field => ["strings011"]
					}
				}
				if "%{[Strings][10]}" in [strings010] {
					mutate {
						remove_field => ["strings010"]
					}
				}
				if "%{[Strings][9]}" in [strings9] {
					mutate {
						remove_field => ["strings9"]
					}
				}
				if "%{[Strings][8]}" in [strings8] {
					mutate {
						remove_field => ["strings8"]
					}
				}
				if "%{[Strings][7]}" in [strings7] {
					mutate {
						remove_field => ["strings7"]
					}
				}
				if "%{[Strings][6]}" in [strings6] {
					mutate {
						remove_field => ["strings6"]
					}
				}
				if "%{[Strings][5]}" in [strings5] {
					mutate {
						remove_field => ["strings5"]
					}
				}
				if "%{[Strings][4]}" in [strings4] {
					mutate {
						remove_field => ["strings4"]
					}
				}
				if "%{[Strings][3]}" in [strings3] {
					mutate {
						remove_field => ["strings3"]
					}
				}
				if "%{[Strings][2]}" in [strings2] {
					mutate {
						remove_field => ["strings2"]
					}
				}
				if "%{[Strings][1]}" in [strings1] {
					mutate {
						remove_field => ["strings1"]
					}
				}
				if "%{[Strings][0]}" in [strings0] {
					mutate {
						remove_field => ["strings0"]
					}
				}
			#Done Son
			######POSTPROCESSING
				#Drop the CSV and Date Parser Failure Logs
				if "_csvparsefailure" in [tags] {
					drop {}
					}
				if "_dateparsefailure" in [tags] {
					drop {}
					}		
			}
	}
}
#=-=--=-=-=-=-=-=-=-==--=-====
#
#	SecurityLogs OUTPUT
#
#-=---=-=-=-=-=-=-=-=-=-=-=-=-
output {
	##### USE the STDOUT to debug the this configuration. If debugging Comment out the entire output except for the STDOUT line
	#stdout { codec => rubydebug }
	
	#### When Hosts contains multiple IP addresses below to ouput to multiple Elasticsearch instances
	#### Logstash Load Balances the output the Clustered Elasticsearch Nodes (hence 4 IPs in my output)
	
	if [type] == "SecurityLogs" and "error" in [tags]{
		elasticsearch {
			hosts => ["10.20.52.123:9200","10.20.52.124:9200","10.20.52.125:9200","10.20.52.126:9200"]
			index => "logstash-errorevt-%{+YYY.MM.dd}"
		}
	}
	if [type] == "SecurityLogs" and "error" not in [tags]{
		elasticsearch {
			hosts => ["10.20.52.123:9200","10.20.52.124:9200","10.20.52.125:9200","10.20.52.126:9200"]
			index => "logstash-winevtx-%{+YYY.MM.dd}"
		}
	}	
}	
